import * as fs from "fs";
import * as path from "path";
import * as dotenv from "dotenv";
import { ConvexHttpClient } from "convex/browser";
import { api } from "../convex/_generated/api";

dotenv.config({ path: ".env.local" });

type CourseSeed = {
  title: string;
  slug: string;
  description: string;
  icon: string;
  order: number;
};

type ChallengeSeed = {
  slug: string;
  title: string;
  description: string;
  category: string;
  difficulty: number;
  xpReward: number;
  order: number;
  type: "theory";
  theoryContent: string;
  hints: string[];
};

type GeneratedBundle = {
  generatedAt: string;
  courses: Array<{
    course: CourseSeed;
    challenges: ChallengeSeed[];
  }>;
};

function md(...blocks: string[]) {
  return blocks
    .map((b) => b.trimEnd())
    .join("\n\n")
    .trim()
    .concat("\n");
}

function interactive(spec: unknown) {
  return md("```interactive", JSON.stringify(spec, null, 2), "```");
}

function matrixTransform(spec: unknown) {
  return md("```matrix-transform", JSON.stringify(spec, null, 2), "```");
}

function buildBundle(): GeneratedBundle {
  const linearAlgebra: CourseSeed = {
    title: "Linear Algebra",
    slug: "linear-algebra",
    description:
      "Vectors, matrices, linear maps, eigenvalues — with geometric intuition and hands-on interactive micro-labs.",
    icon: "Grid2x2",
    order: 20,
  };

  const calculus1: CourseSeed = {
    title: "Calculus I",
    slug: "calculus-1",
    description:
      "Limits → derivatives → integrals, with a focus on meaning, estimation, and the Fundamental Theorem of Calculus.",
    icon: "FunctionSquare",
    order: 21,
  };

  const probability: CourseSeed = {
    title: "Probability",
    slug: "probability",
    description:
      "Axioms, counting, conditional probability, random variables, and normal intuition — with simulations and Bayes.",
    icon: "Dice5",
    order: 22,
  };

  const laModules: ChallengeSeed[] = [
    {
      slug: "linear-algebra-01-vectors-dot-products",
      title: "Vectors, Norms, and Dot Products",
      description: "Geometric meaning of $u\\cdot v$, length, angle, and projection.",
      category: linearAlgebra.title,
      difficulty: 1,
      xpReward: 140,
      order: 1,
      type: "theory",
      theoryContent: md(
        "## The objects of the course",
        "A **vector** is an element of $\\mathbb{R}^n$. Think “arrow” (geometry) and “list of coordinates” (algebra). We’ll switch views constantly.",
        "For $u=(u_1,u_2)$ and $v=(v_1,v_2)$ in $\\mathbb{R}^2$, the **dot product** is:",
        "$$u\\cdot v = u_1v_1+u_2v_2.$$",
        "The **norm** (length) is:",
        "$$\\|u\\|=\\sqrt{u\\cdot u}=\\sqrt{u_1^2+u_2^2}.$$",
        "The dot product encodes the angle $\\theta$ between $u$ and $v$:",
        "$$u\\cdot v = \\|u\\|\\,\\|v\\|\\cos\\theta,\\qquad \\cos\\theta=\\frac{u\\cdot v}{\\|u\\|\\,\\|v\\|}.$$",
        "Two vectors are **orthogonal** iff $u\\cdot v=0$.",
        "### Projection: extracting the component along a direction",
        "If $v\\neq 0$, the projection of $u$ onto $v$ is",
        "$$\\mathrm{proj}_v(u)=\\left(\\frac{u\\cdot v}{v\\cdot v}\\right)v.$$",
        "This is the unique vector in $\\mathrm{span}\\{v\\}$ closest to $u$.",
        "### Inequalities you will use everywhere",
        "Cauchy–Schwarz:",
        "$$|u\\cdot v|\\le \\|u\\|\\,\\|v\\|.$$",
        "Triangle inequality:",
        "$$\\|u+v\\|\\le \\|u\\|+\\|v\\|.$$",
        "### Interactive lab",
        "Try to make the dot product positive/negative/zero by changing the vectors.",
        interactive({ kind: "dot2D", title: "Dot product lab", u: [2, 1], v: [1, 2] }),
        "And now connect the algebra to geometry: drag the vector and observe how a matrix maps it.",
        matrixTransform({ initialPreset: "identity", initialVector: { x: 2, y: 1 } }),
        "### Quick checks",
        "- If $u\\cdot v<0$, is $\\theta$ acute, right, or obtuse?",
        "- Compute $\\mathrm{proj}_{(1,1)}(2,0)$ by hand.",
        "- Show that $\\|u\\|=\\|{-}u\\|$.",
      ),
      hints: [
        "Remember $u\\cdot v$ is a scalar, not a vector.",
        "Use $u\\cdot v = \\|u\\|\\,\\|v\\|\\cos\\theta$ to interpret signs.",
        "Projection onto $v$ is a scalar multiple of $v$.",
      ],
    },
    {
      slug: "linear-algebra-02-matrices-linear-maps",
      title: "Matrices as Linear Maps",
      description: "Columns, matrix multiplication, and composition of transformations.",
      category: linearAlgebra.title,
      difficulty: 1,
      xpReward: 150,
      order: 2,
      type: "theory",
      theoryContent: md(
        "## What a matrix *does*",
        "A matrix $A\\in\\mathbb{R}^{m\\times n}$ defines a linear map $T(x)=Ax$.",
        "For a $2\\times2$ matrix, the columns are the images of the standard basis:",
        "$$A\\begin{pmatrix}1\\\\0\\end{pmatrix}=\\text{col}_1(A),\\qquad A\\begin{pmatrix}0\\\\1\\end{pmatrix}=\\text{col}_2(A).$$",
        "That single fact explains most of “matrix intuition”.",
        "### Matrix multiplication is composition",
        "If $T(x)=Ax$ and $S(x)=Bx$, then $T\\circ S(x)=A(Bx)=(AB)x$.",
        "So multiplication is not “just a formula” — it is function composition. That’s why it is generally **not commutative**: $AB\\neq BA$.",
        "### Interactive lab: compute AB, then visualize",
        interactive({
          kind: "matrixMultiply",
          title: "Compute AB (2×2)",
          a: [
            [1, 2],
            [0, 1],
          ],
          b: [
            [2, 0],
            [1, 1],
          ],
        }),
        "Now apply a matrix to the plane: columns become the new coordinate directions.",
        matrixTransform({ initialPreset: "shear", initialVector: { x: 2, y: 1 } }),
        "### Key formulas",
        "Linearity means:",
        "$$A(u+v)=Au+Av,\\qquad A(cu)=c(Au).$$",
        "And when you write $x$ in coordinates $x=x_1e_1+x_2e_2$, you get:",
        "$$Ax=x_1Ae_1+x_2Ae_2=x_1\\text{col}_1(A)+x_2\\text{col}_2(A).$$",
        "### Quick checks",
        "- Explain in one sentence why $AB\\neq BA$ is natural.",
        "- If $A$ maps $e_1$ to $(3,0)$ and $e_2$ to $(0,2)$, what does it do to $(1,1)$?",
        "- If $A$ is a rotation, what is $\\|Ax\\|$ compared to $\\|x\\|$?",
      ),
      hints: [
        "Think “columns” for $Ax$ in $\\mathbb{R}^2$.",
        "Multiplication corresponds to doing one transform after another.",
        "Test commutativity by choosing simple shear/scale matrices.",
      ],
    },
    {
      slug: "linear-algebra-03-linear-systems-cramers-rule",
      title: "Solving 2×2 Systems (and why det matters)",
      description: "Existence/uniqueness via $\\det(A)$; solve with Cramer's rule.",
      category: linearAlgebra.title,
      difficulty: 1,
      xpReward: 150,
      order: 3,
      type: "theory",
      theoryContent: md(
        "## Linear systems as geometry",
        "A system",
        "$$\\begin{cases}a_{11}x+a_{12}y=b_1\\\\a_{21}x+a_{22}y=b_2\\end{cases}$$",
        "is the intersection of two lines in the plane.",
        "Write it as $Ax=b$ with",
        "$$A=\\begin{pmatrix}a_{11}&a_{12}\\\\a_{21}&a_{22}\\end{pmatrix},\\quad x=\\begin{pmatrix}x\\\\y\\end{pmatrix},\\quad b=\\begin{pmatrix}b_1\\\\b_2\\end{pmatrix}.$$",
        "### The determinant decides “unique or not” (in 2×2)",
        "$$\\det(A)=a_{11}a_{22}-a_{12}a_{21}.$$",
        "- If $\\det(A)\\neq 0$, the lines cross at exactly one point (unique solution).",
        "- If $\\det(A)=0$, the lines are parallel or identical (no solution or infinitely many).",
        "### Interactive lab",
        interactive({
          kind: "linearSystem2x2",
          title: "Solve Ax=b instantly",
          A: [
            [2, 1],
            [1, 3],
          ],
          b: [1, 2],
        }),
        "### Connection to the inverse",
        "When $\\det(A)\\neq 0$, the inverse exists and $x=A^{-1}b$.",
        "For $2\\times 2$,",
        "$$A^{-1}=\\frac{1}{\\det(A)}\\begin{pmatrix}a_{22}&-a_{12}\\\\-a_{21}&a_{11}\\end{pmatrix}.$$",
        "### Quick checks",
        "- Give a geometric reason why $\\det(A)=0$ means no unique solution.",
        "- Solve $\\begin{cases}x+y=2\\\\2x+2y=4\\end{cases}$: what happens and why?",
        "- If $A$ is invertible, show that $Ax=0$ implies $x=0$.",
      ),
      hints: [
        "Unique solutions correspond to invertible matrices.",
        "In 2D, $\\det(A)=0$ means the columns are linearly dependent.",
        "Try making one row a multiple of another to force $\\det(A)=0$.",
      ],
    },
    {
      slug: "linear-algebra-04-determinants-geometry",
      title: "Determinants: Area, Orientation, Invertibility",
      description: "Interpret $\\det(A)$ as signed area scaling for 2×2 maps.",
      category: linearAlgebra.title,
      difficulty: 1,
      xpReward: 155,
      order: 4,
      type: "theory",
      theoryContent: md(
        "## Determinant as scaling factor",
        "For $A\\in\\mathbb{R}^{2\\times 2}$, the determinant measures how areas scale under $x\\mapsto Ax$:",
        "$$\\text{area}(A\\,R)=|\\det(A)|\\,\\text{area}(R).$$",
        "It also records orientation: $\\det(A)<0$ flips orientation.",
        "### Algebraic definition (2×2)",
        "$$\\det\\begin{pmatrix}a&b\\\\c&d\\end{pmatrix}=ad-bc.$$",
        "### Interactive lab",
        "Tune the matrix until it becomes singular ($\\det=0$), and watch the inverse disappear.",
        interactive({
          kind: "determinant2x2",
          title: "Determinant and inverse (2×2)",
          A: [
            [2, 1],
            [3, 2],
          ],
        }),
        "Now visualize a shear/rotation/scale and check how $\\det(A)$ behaves.",
        matrixTransform({ initialPreset: "rotate45", initialVector: { x: 2, y: 1 } }),
        "### Quick checks",
        "- What does $\\det(A)=1$ mean geometrically?",
        "- Why can a matrix with $\\det(A)=0$ not be invertible?",
        "- If $A$ is a reflection, what do you expect $\\det(A)$ to be?",
      ),
      hints: [
        "Invertible iff determinant is nonzero (in finite dimensions).",
        "Rotation preserves area, so |det| should be 1.",
        "Shears preserve area too (det = 1) even though shapes distort.",
      ],
    },
    {
      slug: "linear-algebra-05-vector-spaces-basis-dimension",
      title: "Vector Spaces, Span, Basis, Dimension",
      description: "What “span” means, and how a basis gives coordinates.",
      category: linearAlgebra.title,
      difficulty: 1,
      xpReward: 155,
      order: 5,
      type: "theory",
      theoryContent: md(
        "## Span and subspaces",
        "Given vectors $v_1,\\dots,v_k\\in\\mathbb{R}^n$, their span is",
        "$$\\mathrm{span}\\{v_1,\\dots,v_k\\}=\\left\\{\\sum_{i=1}^k c_iv_i\\,:\\,c_i\\in\\mathbb{R}\\right\\}.$$",
        "A **subspace** is a set closed under addition and scalar multiplication.",
        "### Basis and dimension",
        "A set $\\{b_1,\\dots,b_r\\}$ is a **basis** of a subspace $V$ if:",
        "1) it spans $V$, and 2) it is linearly independent.",
        "Then every $v\\in V$ has a unique coordinate representation",
        "$$v=\\alpha_1b_1+\\cdots+\\alpha_rb_r.$$",
        "The number $r$ is the **dimension**: $\\dim(V)=r$.",
        "### Columns = images of basis vectors",
        "In $\\mathbb{R}^2$, the columns of $A$ are $Ae_1$ and $Ae_2$. So changing columns literally changes the image of the standard basis.",
        "### Interactive lab",
        "Use the visualizer to see how columns generate the transformed grid (the whole plane if $\\det\\neq 0$).",
        matrixTransform({
          initialMatrix: [
            [1, 2],
            [0, 1],
          ],
          initialVector: { x: 1, y: 1 },
        }),
        "Now connect “span” to solving for coefficients: solve for $(x,y)$ in $xv_1+yv_2=b$ by choosing $A=[v_1\\ v_2]$.",
        interactive({
          kind: "linearSystem2x2",
          title: "Coordinates in a basis (solve x v1 + y v2 = b)",
          A: [
            [1, 2],
            [0, 1],
          ],
          b: [3, 2],
        }),
        "### Quick checks",
        "- Give an example of a subset of $\\mathbb{R}^2$ that is *not* a subspace.",
        "- Why can’t a basis contain the zero vector?",
        "- If $A$ is invertible, what is the column space of $A$ in $\\mathbb{R}^2$?",
      ),
      hints: [
        "A subspace must contain 0 (set c=0).",
        "Independence means no vector is a combination of the others.",
        "Invertible 2×2 matrices map the plane to the whole plane.",
      ],
    },
    {
      slug: "linear-algebra-06-rank-nullspace-imt",
      title: "Rank, Null Space, and the Invertible Matrix Theorem",
      description: "How many degrees of freedom remain after applying A?",
      category: linearAlgebra.title,
      difficulty: 1,
      xpReward: 160,
      order: 6,
      type: "theory",
      theoryContent: md(
        "## Two fundamental subspaces",
        "For a matrix $A\\in\\mathbb{R}^{m\\times n}$:",
        "- The **column space** is $\\mathcal{C}(A)=\\{Ax: x\\in\\mathbb{R}^n\\}\\subseteq\\mathbb{R}^m$.",
        "- The **null space** is $\\mathcal{N}(A)=\\{x\\in\\mathbb{R}^n: Ax=0\\}\\subseteq\\mathbb{R}^n$.",
        "Their dimensions are **rank** and **nullity**. The rank–nullity theorem states:",
        "$$\\mathrm{rank}(A)+\\mathrm{nullity}(A)=n.$$",
        "### Invertible Matrix Theorem (finite-dimensional punchline)",
        "For an $n\\times n$ matrix $A$, the following are equivalent:",
        "- $A$ is invertible",
        "- $Ax=0$ has only the trivial solution",
        "- columns of $A$ are linearly independent",
        "- $\\det(A)\\neq 0$",
        "- $\\mathrm{rank}(A)=n$",
        "- $Ax=b$ has a unique solution for every $b$",
        "and several more.",
        "### Interactive lab: make the system lose uniqueness",
        "First, watch $\\det(A)$:",
        interactive({
          kind: "determinant2x2",
          title: "Singularity detector",
          A: [
            [1, 2],
            [2, 4],
          ],
        }),
        "Then, solve $Ax=b$ for a singular matrix and notice the “no unique solution” behavior.",
        interactive({
          kind: "linearSystem2x2",
          title: "Try solving with det(A)=0",
          A: [
            [1, 2],
            [2, 4],
          ],
          b: [1, 2],
        }),
        "### Quick checks",
        "- If $Ax=0$ has a nonzero solution, what must be true about $\\det(A)$ (for square A)?",
        "- In $\\mathbb{R}^2$, what does rank 1 look like geometrically?",
        "- Give a sentence interpretation of rank–nullity.",
      ),
      hints: [
        "Rank is “how many independent outputs” the matrix can produce.",
        "Nullity is “how many input directions collapse to 0”.",
        "Singular means some nonzero vector gets mapped to 0 or to a lower-dimensional set.",
      ],
    },
    {
      slug: "linear-algebra-07-eigenvalues-eigenvectors",
      title: "Eigenvalues and Eigenvectors",
      description: "Directions that only stretch/flip: $Av=\\lambda v$.",
      category: linearAlgebra.title,
      difficulty: 1,
      xpReward: 165,
      order: 7,
      type: "theory",
      theoryContent: md(
        "## The eigen-equation",
        "An eigenvector $v\\neq 0$ and eigenvalue $\\lambda$ satisfy",
        "$$Av=\\lambda v.$$",
        "Interpretation: $v$ points along a direction that the transformation preserves (up to scaling).",
        "### How to compute: characteristic polynomial",
        "Solve",
        "$$\\det(A-\\lambda I)=0.$$",
        "For $2\\times 2$,",
        "$$\\det(A-\\lambda I)=\\lambda^2-(\\mathrm{tr}\\,A)\\lambda+\\det(A).$$",
        "So eigenvalues depend only on trace and determinant.",
        "### Interactive lab",
        interactive({
          kind: "eigen2x2",
          title: "Eigenvalues (2×2) from trace/det",
          A: [
            [2, 1],
            [1, 2],
          ],
        }),
        "Use the matrix transform visualizer and look for directions that keep their line (eigenvectors).",
        matrixTransform({
          initialMatrix: [
            [2, 1],
            [1, 2],
          ],
          initialVector: { x: 1, y: 0 },
        }),
        "### Quick checks",
        "- If $A$ is a pure rotation by 45°, why do you expect no real eigenvectors?",
        "- For symmetric $A$, what special property do eigenvectors have (orthogonality)?",
        "- If $A$ has two independent eigenvectors, what form can you write $A$ in?",
      ),
      hints: [
        "Rotation matrices have complex eigenvalues unless rotation is 0° or 180°.",
        "Symmetric matrices have real eigenvalues and orthogonal eigenvectors.",
        "Diagonalization is $A=PDP^{-1}$ when enough eigenvectors exist.",
      ],
    },
    {
      slug: "linear-algebra-08-orthogonality-projections",
      title: "Orthogonality, Projections, and Best Approximation",
      description: "Orthogonal decomposition and why projections solve “closest point” problems.",
      category: linearAlgebra.title,
      difficulty: 1,
      xpReward: 165,
      order: 8,
      type: "theory",
      theoryContent: md(
        "## Orthogonality is structure",
        "A set $\\{q_1,\\dots,q_k\\}$ is orthonormal if $q_i\\cdot q_j=\\delta_{ij}$.",
        "When you have an orthonormal basis, coordinates become dot products:",
        "$$x=\\sum_{i=1}^k (x\\cdot q_i)q_i.$$",
        "### Projection revisited (subspace version)",
        "If $Q$ has orthonormal columns spanning a subspace $V$, then the projection onto $V$ is",
        "$$\\mathrm{proj}_V(x)=QQ^Tx.$$",
        "The residual $x-\\mathrm{proj}_V(x)$ is orthogonal to $V$.",
        "### Interactive lab",
        interactive({ kind: "dot2D", title: "Make vectors orthogonal", u: [2, 1], v: [1, -2] }),
        "Try a reflection: reflections preserve lengths but flip orientation (det = -1).",
        matrixTransform({ initialPreset: "reflectX", initialVector: { x: 2, y: 1 } }),
        "### Quick checks",
        "- If $q_1,q_2$ are orthonormal, compute the projection of $x$ onto their span.",
        "- Why does orthogonality make least squares solvable with dot products?",
        "- Show $\\|x\\|^2=\\|\\mathrm{proj}_V(x)\\|^2+\\|x-\\mathrm{proj}_V(x)\\|^2$ (Pythagoras).",
      ),
      hints: [
        "Orthonormal means dot products are 0 off-diagonal and 1 on-diagonal.",
        "Projections are best approximations because the error is orthogonal.",
        "Think of decomposing x into parallel + perpendicular components.",
      ],
    },
    {
      slug: "linear-algebra-09-least-squares-normal-equations",
      title: "Least Squares and the Normal Equations (2D intuition)",
      description: "When Ax=b is inconsistent, solve the “closest” problem instead.",
      category: linearAlgebra.title,
      difficulty: 1,
      xpReward: 170,
      order: 9,
      type: "theory",
      theoryContent: md(
        "## The problem",
        "If $Ax=b$ has no exact solution, we instead minimize the squared error:",
        "$$\\min_x \\|Ax-b\\|^2.$$",
        "The minimizer $\\hat x$ solves the **normal equations**:",
        "$$A^TA\\hat x=A^Tb.$$",
        "Geometric meaning: $A\\hat x$ is the projection of $b$ onto the column space of $A$.",
        "### 2×2 lab (a gentle gateway)",
        "When $A$ is invertible, the normal equations reduce to the usual solution. Use this as a sanity check.",
        interactive({
          kind: "linearSystem2x2",
          title: "Solve Ax=b (invertible case as baseline)",
          A: [
            [1, 1],
            [1, 2],
          ],
          b: [2, 3],
        }),
        "Now interpret columns as spanning directions: changing columns changes what can be fit.",
        matrixTransform({
          initialMatrix: [
            [1, 1],
            [1, 2],
          ],
          initialVector: { x: 1, y: 1 },
        }),
        "### Quick checks",
        "- Why is $A^TA$ always symmetric?",
        "- If columns of $A$ are independent, why is $A^TA$ invertible?",
        "- What does it mean geometrically that $A^T(A\\hat x-b)=0$?",
      ),
      hints: [
        "Differentiate \\|Ax-b\\|^2 with respect to x to get normal equations.",
        "A^T(Ax-b)=0 says residual is orthogonal to column space.",
        "A^TA is positive semidefinite; positive definite if columns independent.",
      ],
    },
    {
      slug: "linear-algebra-10-symmetric-matrices-quadratic-forms",
      title: "Symmetric Matrices and Quadratic Forms",
      description: "Eigenvalues as curvature: $x^TAx$ and principal directions.",
      category: linearAlgebra.title,
      difficulty: 1,
      xpReward: 175,
      order: 10,
      type: "theory",
      theoryContent: md(
        "## Quadratic forms",
        "Given symmetric $A$, the scalar",
        "$$q(x)=x^TAx$$",
        "describes “curvature” and energy-like quantities.",
        "A key theorem: symmetric matrices have an orthonormal eigenbasis, and",
        "$$A=Q\\Lambda Q^T,$$",
        "so",
        "$$x^TAx=(Q^Tx)^T\\Lambda(Q^Tx)=\\sum_{i=1}^n \\lambda_i y_i^2.$$\n",
        "This is why eigenvalues control whether $q(x)$ is always positive/negative/indefinite.",
        "### Interactive lab",
        interactive({
          kind: "eigen2x2",
          title: "Eigenvalues of a symmetric matrix",
          A: [
            [3, 1],
            [1, 2],
          ],
        }),
        "### Quick checks",
        "- If all eigenvalues are positive, what can you say about $x^TAx$?",
        "- Why do symmetric matrices have real eigenvalues?",
        "- How does diagonalization simplify computing $A^k$?",
      ),
      hints: [
        "Positive definite iff all eigenvalues > 0.",
        "Symmetric implies orthogonal diagonalization.",
        "In the eigenbasis, A acts like scaling by \\lambda_i on each coordinate axis.",
      ],
    },
  ];

  const calcModules: ChallengeSeed[] = [
    {
      slug: "calculus-1-01-limits-continuity",
      title: "Limits and Continuity",
      description: "Approach a point from both sides: the idea behind $\\lim_{x\\to a} f(x)$.",
      category: calculus1.title,
      difficulty: 1,
      xpReward: 140,
      order: 1,
      type: "theory",
      theoryContent: md(
        "## The central question",
        "A limit asks: what does $f(x)$ approach as $x$ gets close to $a$?",
        "$$\\lim_{x\\to a} f(x)=L$$",
        "means: make $f(x)$ as close to $L$ as we want by taking $x$ sufficiently close to $a$ (but not equal).",
        "### (Optional) epsilon–delta definition",
        "$$\\forall\\varepsilon>0\\ \\exists\\delta>0\\ \\text{s.t. }0<|x-a|<\\delta\\Rightarrow |f(x)-L|<\\varepsilon.$$",
        "This is the “programmable” definition: it’s a specification, not a trick.",
        "### Continuity",
        "$f$ is continuous at $a$ if",
        "$$\\lim_{x\\to a} f(x)=f(a).$$",
        "### Interactive lab: a limit table",
        interactive({
          kind: "limitTable",
          title: "Look at f(a±h) as h→0",
          function: "sin(x)",
          a: 0,
        }),
        "### Quick checks",
        "- Compute $\\lim_{x\\to 0} \\frac{\\sin x}{x}$ (a famous one) and explain why it matters.",
        "- Give an example where the limit exists but $f(a)$ is undefined.",
        "- Give an example where $f(a)$ exists but the limit does not.",
      ),
      hints: [
        "Limits care about *nearby* x-values, not the value at a.",
        "Two-sided limit exists iff left and right limits match.",
        "Continuity is “limit equals value”.",
      ],
    },
    {
      slug: "calculus-1-02-derivative-definition",
      title: "Derivatives from First Principles",
      description: "Instantaneous rate of change: $f'(a)=\\lim_{h\\to 0}\\frac{f(a+h)-f(a)}{h}$.",
      category: calculus1.title,
      difficulty: 1,
      xpReward: 150,
      order: 2,
      type: "theory",
      theoryContent: md(
        "## The derivative is a limit",
        "The derivative at $x=a$ is defined by",
        "$$f'(a)=\\lim_{h\\to 0}\\frac{f(a+h)-f(a)}{h}.$$",
        "Interpretations:",
        "- **Slope** of the tangent line.",
        "- **Instantaneous rate** (velocity, marginal cost, etc.).",
        "### Rules (to be justified, then used)",
        "If $c$ is constant:",
        "- $(c)'=0$",
        "- $(x^n)'=nx^{n-1}$ (power rule)",
        "- $(f+g)'=f'+g'$",
        "- $(fg)'=f'g+fg'$ (product rule)",
        "- $\\left(\\frac{f}{g}\\right)'=\\frac{f'g-fg'}{g^2}$ (quotient rule)",
        "- $(f\\circ g)'=(f'\\circ g)\\cdot g'$ (chain rule)",
        "### Interactive lab: difference quotient",
        interactive({
          kind: "derivativeDQ",
          title: "Watch DQ → f'(x0) as h→0",
          function: "x^2",
          x0: 1,
          h: 0.1,
        }),
        "### Quick checks",
        "- Use the definition to compute the derivative of $f(x)=x^2$ at $x=1$.",
        "- Why does decreasing $h$ too far sometimes cause numerical issues (roundoff)?",
        "- Give a physical example where derivative makes sense.",
      ),
      hints: [
        "The derivative is a *limit* of slopes of secant lines.",
        "Use algebra to simplify the difference quotient before taking h→0.",
        "Chain rule handles “inside functions”.",
      ],
    },
    {
      slug: "calculus-1-03-derivative-applications",
      title: "Derivative Applications: Optimization and Linearization",
      description:
        "Use $f'(x)=0$ and sign changes to locate extrema; approximate locally with tangent lines.",
      category: calculus1.title,
      difficulty: 1,
      xpReward: 155,
      order: 3,
      type: "theory",
      theoryContent: md(
        "## Optimization recipe",
        "To maximize/minimize $f$ on an interval:",
        "1) Find critical points where $f'(x)=0$ or $f'$ undefined.\n2) Check endpoints.\n3) Compare values.",
        "Second derivative intuition:",
        "$$f''(x)>0\\Rightarrow \\text{concave up (min-ish)},\\qquad f''(x)<0\\Rightarrow \\text{concave down (max-ish)}.$$",
        "## Linearization (tangent-line approximation)",
        "Near $x=a$:",
        "$$f(x)\\approx f(a)+f'(a)(x-a).$$",
        "This is the first-order Taylor polynomial.",
        "### Interactive lab: local slope at different x0",
        interactive({
          kind: "derivativeDQ",
          title: "Try different x0 and h",
          function: "x^3",
          x0: 1,
          h: 0.2,
        }),
        "### Quick checks",
        "- Use linearization to approximate $\\sqrt{4.1}$ using $f(x)=\\sqrt{x}$ at $a=4$.",
        "- Explain why $f'(x)=0$ is necessary (but not sufficient) for a local extremum.",
        "- Give an example where $f'(x)=0$ but there is no extremum (an inflection point).",
      ),
      hints: [
        "Linearization is just the tangent line.",
        "Critical points also include where derivative does not exist.",
        "Always check endpoints on closed intervals.",
      ],
    },
    {
      slug: "calculus-1-04-accumulation-and-integral",
      title: "Integrals as Accumulation",
      description: "Area under a curve via limits of sums: $\\int_a^b f(x)\\,dx$.",
      category: calculus1.title,
      difficulty: 1,
      xpReward: 155,
      order: 4,
      type: "theory",
      theoryContent: md(
        "## Riemann sums",
        "Partition $[a,b]$ into $n$ pieces of width $\\Delta x=\\frac{b-a}{n}$ and choose sample points $x_i^*$. Then",
        "$$\\sum_{i=1}^n f(x_i^*)\\,\\Delta x$$",
        "approximates the accumulated quantity (area if $f\\ge 0$).",
        "The definite integral is the limit (when it exists):",
        "$$\\int_a^b f(x)\\,dx=\\lim_{n\\to\\infty}\\sum_{i=1}^n f(x_i^*)\\,\\Delta x.$$",
        "### Properties",
        "- Linearity: $\\int (af+bg)=a\\int f + b\\int g$.\n- Additivity: $\\int_a^b f=\\int_a^c f+\\int_c^b f$.",
        "### Interactive lab: Riemann sums",
        interactive({
          kind: "riemannSum",
          title: "Approximate an integral",
          function: "x^2",
          a: 0,
          b: 1,
          n: 10,
          method: "midpoint",
        }),
        "### Quick checks",
        "- Compute $\\int_0^1 x\\,dx$ by geometry and compare to a Riemann sum.",
        "- If $f$ can be negative, what does $\\int_a^b f$ represent?",
        "- Why does increasing $n$ usually improve the approximation?",
      ),
      hints: [
        "Integral is a limit of sums.",
        "Negative area counts with a minus sign (signed area).",
        "Midpoint rules often converge faster than left/right.",
      ],
    },
    {
      slug: "calculus-1-05-fundamental-theorem",
      title: "The Fundamental Theorem of Calculus",
      description: "Derivatives and integrals are inverse processes (under hypotheses).",
      category: calculus1.title,
      difficulty: 1,
      xpReward: 160,
      order: 5,
      type: "theory",
      theoryContent: md(
        "## FTC Part I (accumulation has a derivative)",
        "Define",
        "$$F(x)=\\int_a^x f(t)\\,dt.$$",
        "Then (under mild conditions) $F'(x)=f(x)$.",
        "## FTC Part II (evaluate integrals with antiderivatives)",
        "If $F'=f$, then",
        "$$\\int_a^b f(x)\\,dx = F(b)-F(a).$$",
        "This converts geometry/accumulation into algebra.",
        "### Interactive lab: approximate vs exact intuition",
        interactive({
          kind: "riemannSum",
          title: "Try sin(x) on [0, π]",
          function: "sin(x)",
          a: 0,
          b: 3.1415926535,
          n: 20,
          method: "midpoint",
        }),
        "You should expect $\\int_0^{\\pi} \\sin x\\,dx=2$ because an antiderivative is $-\\cos x$:",
        "$$\\int_0^{\\pi}\\sin x\\,dx=[-\\cos x]_0^{\\pi}=(-\\cos\\pi)-(-\\cos 0)=1-(-1)=2.$$",
        "### Quick checks",
        "- Explain (in words) why Part I implies Part II.\n- If $f$ is discontinuous, what can go wrong?\n- Differentiate $F(x)=\\int_1^x t^3\\,dt$.",
      ),
      hints: [
        "Part I: the accumulation function changes at rate f.",
        "Part II: antiderivative evaluates net accumulation.",
        "Be careful with variable names inside integrals.",
      ],
    },
    {
      slug: "calculus-1-06-average-value-area-between",
      title: "Applications of Integrals: Average Value and Area",
      description: "Compute mean values and areas between curves with definite integrals.",
      category: calculus1.title,
      difficulty: 1,
      xpReward: 160,
      order: 6,
      type: "theory",
      theoryContent: md(
        "## Average value",
        "The average value of $f$ on $[a,b]$ is",
        "$$f_{\\text{avg}}=\\frac{1}{b-a}\\int_a^b f(x)\\,dx.$$",
        "## Area between curves",
        "If $f(x)\\ge g(x)$ on $[a,b]$, then",
        "$$\\text{Area}=\\int_a^b (f(x)-g(x))\\,dx.$$",
        "### Interactive lab: average value intuition",
        "Approximate $\\int_0^1 x^2\\,dx$ and then compute $f_{\\text{avg}}$.",
        interactive({
          kind: "riemannSum",
          title: "Riemann sum for x^2",
          function: "x^2",
          a: 0,
          b: 1,
          n: 50,
          method: "midpoint",
        }),
        "Exact value is $\\int_0^1 x^2\\,dx=\\frac{1}{3}$, so $f_{\\text{avg}}=\\frac{1}{1-0}\\cdot\\frac{1}{3}=\\frac{1}{3}$.",
        "### Quick checks",
        "- If $f$ is constant, does $f_{\\text{avg}}$ equal that constant?\n- Interpret average value physically (average velocity, average density).\n- Why must you ensure which curve is on top for area?",
      ),
      hints: [
        "Average value is “total / length of interval”.",
        "Area between curves is integral of top minus bottom.",
        "If curves cross, split the interval at intersection points.",
      ],
    },
    {
      slug: "calculus-1-07-sequences-series-intro",
      title: "Sequences and Infinite Series (Intro)",
      description: "Convergence as a limit; series as sums of sequences of partial sums.",
      category: calculus1.title,
      difficulty: 1,
      xpReward: 165,
      order: 7,
      type: "theory",
      theoryContent: md(
        "## Sequences",
        "A sequence $(a_n)$ converges to $L$ if",
        "$$\\lim_{n\\to\\infty} a_n=L.$$",
        "## Series",
        "A series is the limit of partial sums:",
        "$$\\sum_{n=1}^{\\infty} a_n\\quad\\text{converges if}\\quad S_N=\\sum_{n=1}^N a_n\\ \\text{converges as }N\\to\\infty.$$",
        "Necessary condition: if $\\sum a_n$ converges, then $a_n\\to 0$.",
        "### Geometric series (the key example)",
        "$$\\sum_{n=0}^{\\infty} ar^n=\\frac{a}{1-r}\\quad (|r|<1).$$",
        "### Interactive lab: Taylor/Maclaurin previews",
        "Many functions are represented by infinite series near 0. Explore how partial sums improve.",
        interactive({
          kind: "taylor",
          title: "Partial sums as approximations",
          function: "exp(x)",
          degree: 4,
          x: 1,
        }),
        "### Quick checks",
        "- Why does $a_n\\to 0$ not guarantee $\\sum a_n$ converges? (Give a counterexample.)\n- Compute $\\sum_{n=0}^\\infty (1/2)^n$.\n- What does convergence mean in terms of error $|S_N-S|$?",
      ),
      hints: [
        "Harmonic series: a_n→0 but diverges.",
        "Geometric series has a closed form.",
        "Partial sums give successive approximations to the infinite sum.",
      ],
    },
    {
      slug: "calculus-1-08-taylor-series",
      title: "Taylor Polynomials and Series",
      description: "Approximate functions locally with polynomials; quantify error.",
      category: calculus1.title,
      difficulty: 1,
      xpReward: 165,
      order: 8,
      type: "theory",
      theoryContent: md(
        "## Taylor polynomial",
        "The degree-$n$ Taylor polynomial of $f$ at $a$ is",
        "$$T_n(x)=\\sum_{k=0}^n \\frac{f^{(k)}(a)}{k!}(x-a)^k.$$",
        "Maclaurin is the special case $a=0$.",
        "### Why it works",
        "Near $x=a$, higher powers $(x-a)^k$ get small fast, so a polynomial can approximate $f$ with controllable error.",
        "### Interactive lab",
        interactive({
          kind: "taylor",
          title: "Approximate sin(x) with Taylor polynomials",
          function: "sin(x)",
          degree: 5,
          x: 1,
        }),
        "### Quick checks",
        "- Write the degree-2 Taylor polynomial of $e^x$ at $0$.\n- For $\\ln(1+x)$, why do we require $|x|<1$ for the series?\n- Interpret the remainder term (error) conceptually.",
      ),
      hints: [
        "Taylor coefficients are derivatives at the center point.",
        "Maclaurin series are around 0.",
        "Convergence can depend on x; radius of convergence matters.",
      ],
    },
    {
      slug: "calculus-1-09-improper-integrals",
      title: "Improper Integrals (Conceptual Preview)",
      description: "When intervals are infinite or integrands blow up, use limits again.",
      category: calculus1.title,
      difficulty: 1,
      xpReward: 170,
      order: 9,
      type: "theory",
      theoryContent: md(
        "## Improper integrals are limits",
        "Type I (infinite interval):",
        "$$\\int_a^{\\infty} f(x)\\,dx:=\\lim_{b\\to\\infty}\\int_a^b f(x)\\,dx.$$",
        "Type II (vertical asymptote):",
        "$$\\int_a^b f(x)\\,dx:=\\lim_{t\\to a^+}\\int_t^b f(x)\\,dx$$",
        "when $f$ blows up near $a$.",
        "### Interactive lab: compare finite approximations",
        "Try an exponentially decaying function: the tail should contribute less and less.",
        interactive({
          kind: "riemannSum",
          title: "Approximate ∫_0^2 e^x dx (then think about tails)",
          function: "exp(x)",
          a: 0,
          b: 2,
          n: 40,
          method: "midpoint",
        }),
        "### Quick checks",
        "- Why do we *define* improper integrals via limits rather than a new notion?\n- Give an example of a divergent improper integral.\n- Compute $\\int_1^{\\infty} \\frac{1}{x^2}\\,dx$.",
      ),
      hints: [
        "Improper integrals convert “infinite” into a limit over finite bounds.",
        "Convergence depends on tail behavior (e.g., p-integrals).",
        "Always evaluate the limit; the antiderivative alone is not enough.",
      ],
    },
    {
      slug: "calculus-1-10-differential-equations-preview",
      title: "Differential Equations (First Taste)",
      description: "Solve simple separable growth laws; connect to exponentials.",
      category: calculus1.title,
      difficulty: 1,
      xpReward: 170,
      order: 10,
      type: "theory",
      theoryContent: md(
        "## A separable prototype",
        "The differential equation",
        "$$\\frac{dy}{dx}=ky$$",
        "models growth/decay (population, radioactive decay, continuous compounding).",
        "Separate variables (heuristically):",
        "$$\\frac{1}{y}\\,dy=k\\,dx$$",
        "Integrate:",
        "$$\\ln|y|=kx+C\\quad\\Rightarrow\\quad y=Ce^{kx}.$$",
        "### Interactive lab: exponential derivative check",
        "The defining property of $e^x$ is that its derivative is itself.",
        interactive({
          kind: "derivativeDQ",
          title: "Check that d/dx e^x = e^x numerically",
          function: "exp(x)",
          x0: 0,
          h: 0.1,
        }),
        "### Quick checks",
        "- Solve $y'=2y$ with initial condition $y(0)=3$.\n- Explain why $\\ln|y|$ appears when integrating $\\frac{1}{y}$.\n- How does the sign of k affect long-term behavior?",
      ),
      hints: [
        "Separable means you can rearrange to y-terms on one side, x-terms on the other.",
        "Integrating 1/y gives ln|y|.",
        "Initial conditions determine the constant C.",
      ],
    },
  ];

  const probModules: ChallengeSeed[] = [
    {
      slug: "probability-01-axioms-events",
      title: "Probability Axioms and Events",
      description: "Sample spaces, events, and the Kolmogorov axioms.",
      category: probability.title,
      difficulty: 1,
      xpReward: 140,
      order: 1,
      type: "theory",
      theoryContent: md(
        "## The model",
        "A probability space $(\\Omega,\\mathcal{F},\\Pr)$ consists of:",
        "- sample space $\\Omega$ (all outcomes)\n- events $A\\in\\mathcal{F}$ (sets of outcomes)\n- probability measure $\\Pr$ satisfying axioms",
        "### Axioms",
        "1) $\\Pr(A)\\ge 0$\n2) $\\Pr(\\Omega)=1$\n3) If $A_i$ are disjoint then $\\Pr(\\cup_i A_i)=\\sum_i \\Pr(A_i)$",
        "Useful consequences:",
        "$$\\Pr(A^c)=1-\\Pr(A),\\qquad \\Pr(A\\cup B)=\\Pr(A)+\\Pr(B)-\\Pr(A\\cap B).$$",
        "### Interactive lab: simulate frequencies",
        "Run a Bernoulli simulation and watch how the empirical frequency $\\hat p$ concentrates near $p$ as n increases.",
        interactive({
          kind: "bernoulliSim",
          title: "Empirical frequency as probability",
          p: 0.3,
          n: 50,
          trials: 400,
        }),
        "### Quick checks",
        "- If $A$ and $B$ are disjoint, what is $\\Pr(A\\cap B)$?\n- If $\\Pr(A)=0.2$, what is $\\Pr(A^c)$?\n- Explain why additivity implies $\\Pr(\\emptyset)=0$.",
      ),
      hints: [
        "Events are sets; probability is a function on sets.",
        "Complement rule is immediate from additivity.",
        "Simulation: relative frequency stabilizes as sample size grows.",
      ],
    },
    {
      slug: "probability-02-counting-combinations",
      title: "Counting: Permutations, Combinations, and Binomial Coefficients",
      description: "Counting is probability’s engine: $\\binom{n}{k}$ and friends.",
      category: probability.title,
      difficulty: 1,
      xpReward: 150,
      order: 2,
      type: "theory",
      theoryContent: md(
        "## Two counting principles",
        "- **Rule of product:** if a choice has $m$ options then another has $n$, total $mn$.\n- **Rule of sum:** disjoint cases add.",
        "### Binomial coefficient",
        "$$\\binom{n}{k}=\\frac{n!}{k!(n-k)!}$$",
        "counts the number of size-$k$ subsets of an $n$-element set.",
        "Binomial theorem:",
        "$$(p+(1-p))^n=\\sum_{k=0}^n \\binom{n}{k}p^k(1-p)^{n-k}=1.$$",
        "### Interactive lab: see the PMF emerge from counting",
        interactive({ kind: "binomial", title: "Binomial PMF from combinations", n: 10, p: 0.5 }),
        "### Quick checks",
        "- Interpret $\\binom{n}{k}$ as “choose positions of successes”.\n- Why do probabilities over k sum to 1?\n- Compute $\\binom{5}{2}$.",
      ),
      hints: [
        "n! counts permutations; divide by k!(n-k)! to ignore order within groups.",
        "Binomial theorem ensures PMF sums to 1.",
        "In binomial trials, only the count of successes matters (exchangeability).",
      ],
    },
    {
      slug: "probability-03-conditional-independence",
      title: "Conditional Probability and Independence",
      description: "Update probabilities with information: $\\Pr(A\\mid B)$.",
      category: probability.title,
      difficulty: 1,
      xpReward: 155,
      order: 3,
      type: "theory",
      theoryContent: md(
        "## Definition",
        "If $\\Pr(B)>0$:",
        "$$\\Pr(A\\mid B)=\\frac{\\Pr(A\\cap B)}{\\Pr(B)}.$$",
        "Multiplication rule:",
        "$$\\Pr(A\\cap B)=\\Pr(A\\mid B)\\Pr(B).$$",
        "### Independence",
        "$A$ and $B$ are independent if",
        "$$\\Pr(A\\cap B)=\\Pr(A)\\Pr(B)$$",
        "equivalently $\\Pr(A\\mid B)=\\Pr(A)$ (when defined).",
        "### Interactive lab (Bayes preview)",
        "Play with priors and false positives to see why conditioning can dramatically change belief.",
        interactive({
          kind: "bayes",
          title: "Conditioning changes belief",
          prior: 0.02,
          sensitivity: 0.95,
          specificity: 0.98,
        }),
        "### Quick checks",
        "- Show that if $A$ and $B$ are independent, then $A$ and $B^c$ are independent.\n- Explain why independence is stronger than “disjoint”.\n- Compute $\\Pr(A\\cap B)$ from $\\Pr(A\\mid B)$ and $\\Pr(B)$.",
      ),
      hints: [
        "Conditional probability is “restrict sample space to B”.",
        "Disjoint events cannot both happen; independent events can.",
        "Use the multiplication rule to move between joint and conditional probabilities.",
      ],
    },
    {
      slug: "probability-04-bayes-theorem",
      title: "Bayes' Theorem",
      description: "Flip conditioning: $\\Pr(A\\mid B)$ from $\\Pr(B\\mid A)$.",
      category: probability.title,
      difficulty: 1,
      xpReward: 155,
      order: 4,
      type: "theory",
      theoryContent: md(
        "## The identity",
        "$$\\Pr(A\\mid B)=\\frac{\\Pr(B\\mid A)\\Pr(A)}{\\Pr(B)}.$$",
        "Law of total probability (partition $\\{A, A^c\\}$):",
        "$$\\Pr(B)=\\Pr(B\\mid A)\\Pr(A)+\\Pr(B\\mid A^c)\\Pr(A^c).$$",
        "Combine them to get the famous formula.",
        "### Interactive lab: medical test paradox",
        interactive({
          kind: "bayes",
          title: "Posterior after a positive test",
          prior: 0.01,
          sensitivity: 0.95,
          specificity: 0.98,
        }),
        "### Quick checks",
        "- Explain in words: why does a tiny prior dominate posteriors?\n- If specificity increases, what happens to false positives?\n- Derive Bayes' rule from $\\Pr(A\\cap B)$ written two ways.",
      ),
      hints: [
        "Write P(A∩B)=P(A|B)P(B)=P(B|A)P(A).",
        "P(B) aggregates all ways B can happen.",
        "False positives matter when the condition is rare.",
      ],
    },
    {
      slug: "probability-05-random-variables-expectation",
      title: "Random Variables and Expectation",
      description: "Expectation as a weighted average; linearity as a superpower.",
      category: probability.title,
      difficulty: 1,
      xpReward: 160,
      order: 5,
      type: "theory",
      theoryContent: md(
        "## Discrete expectation",
        "If $X$ takes values $x_i$ with probabilities $p_i$,",
        "$$\\mathbb{E}[X]=\\sum_i x_i p_i.$$",
        "Linearity (always true, no independence needed):",
        "$$\\mathbb{E}[aX+bY]=a\\mathbb{E}[X]+b\\mathbb{E}[Y].$$",
        "### Example: binomial",
        "If $X\\sim\\mathrm{Bin}(n,p)$ then $\\mathbb{E}[X]=np$.",
        "### Interactive lab",
        interactive({ kind: "binomial", title: "Watch mean scale like np", n: 20, p: 0.3 }),
        "### Quick checks",
        "- Compute $\\mathbb{E}[X]$ for a fair die.\n- Explain why linearity is so useful.\n- For $X\\sim\\mathrm{Bin}(n,p)$, why is expectation $np$ intuitive?",
      ),
      hints: [
        "Expectation is a weighted average.",
        "Linearity lets you break complicated X into sums of simpler pieces.",
        "A binomial is a sum of n Bernoulli trials; expected successes add.",
      ],
    },
    {
      slug: "probability-06-variance-spread",
      title: "Variance: Measuring Spread",
      description:
        "Variance is the average squared deviation: $\\mathrm{Var}(X)=\\mathbb{E}[(X-\\mu)^2]$.",
      category: probability.title,
      difficulty: 1,
      xpReward: 160,
      order: 6,
      type: "theory",
      theoryContent: md(
        "## Definition and identities",
        "Let $\\mu=\\mathbb{E}[X]$. Then",
        "$$\\mathrm{Var}(X)=\\mathbb{E}[(X-\\mu)^2]=\\mathbb{E}[X^2]-\\mu^2.$$",
        "Scaling:",
        "$$\\mathrm{Var}(aX+b)=a^2\\mathrm{Var}(X).$$",
        "For $X\\sim\\mathrm{Bin}(n,p)$:",
        "$$\\mathrm{Var}(X)=np(1-p).$$",
        "### Interactive lab",
        interactive({ kind: "binomial", title: "See how variance changes with p", n: 30, p: 0.5 }),
        "### Quick checks",
        "- Why does variance scale with $a^2$?\n- When is binomial variance largest for fixed n?\n- Compare standard deviation to variance (units).",
      ),
      hints: [
        "Variance has squared units; standard deviation fixes units.",
        "For binomial, spread is biggest near p=0.5.",
        "E[X^2]-E[X]^2 is often easiest to compute.",
      ],
    },
    {
      slug: "probability-07-law-of-large-numbers",
      title: "Law of Large Numbers (LLN)",
      description: "Averages stabilize: $\\hat p\\to p$ as $n\\to\\infty$ (in probability).",
      category: probability.title,
      difficulty: 1,
      xpReward: 165,
      order: 7,
      type: "theory",
      theoryContent: md(
        "## The statement (informal)",
        "If $X_1,X_2,\\dots$ are i.i.d. with mean $\\mu$, then the sample average",
        "$$\\bar X_n=\\frac{1}{n}\\sum_{i=1}^n X_i$$",
        "gets close to $\\mu$ for large $n$.",
        "For Bernoulli trials, this is the intuition behind “frequency = probability”.",
        "### Interactive lab",
        "Increase n and watch the variance shrink like $\\frac{1}{n}$.",
        interactive({
          kind: "bernoulliSim",
          title: "LLN: concentrate around p",
          p: 0.3,
          n: 200,
          trials: 600,
        }),
        "### Quick checks",
        "- If variance shrinks like $1/n$, what happens to standard deviation?\n- Why does LLN not say *how fast* convergence is?\n- Give an everyday example where LLN explains stability.",
      ),
      hints: [
        "Std dev scales like 1/sqrt(n).",
        "Concentration results (Chernoff, Hoeffding) quantify rates.",
        "LLN explains why averages are predictable even when single trials aren’t.",
      ],
    },
    {
      slug: "probability-08-normal-z-scores",
      title: "Normal Distribution and z-scores",
      description: "Standardize: $Z=(X-\\mu)/\\sigma$; use $\\Phi$ for probabilities.",
      category: probability.title,
      difficulty: 1,
      xpReward: 165,
      order: 8,
      type: "theory",
      theoryContent: md(
        "## The normal family",
        "We write $X\\sim\\mathcal{N}(\\mu,\\sigma^2)$. Standardizing gives",
        "$$Z=\\frac{X-\\mu}{\\sigma}\\sim\\mathcal{N}(0,1).$$",
        "Then",
        "$$\\Pr(X\\le x)=\\Pr\\left(Z\\le \\frac{x-\\mu}{\\sigma}\\right)=\\Phi\\left(\\frac{x-\\mu}{\\sigma}\\right).$$",
        "### Interactive lab: compute a CDF value",
        interactive({
          kind: "normalCdf",
          title: "Compute Φ(z) for a z-score",
          mu: 0,
          sigma: 1,
          x: 1.96,
        }),
        "### Quick checks",
        "- What is $\\Pr(|Z|\\le 1.96)$ approximately?\n- Why does scaling by $\\sigma$ change units?\n- Interpret z as “how many standard deviations from the mean”.",
      ),
      hints: [
        "Standardizing turns any normal into the standard normal.",
        "Two-sided probability uses symmetry: P(|Z|≤a)=2Φ(a)-1.",
        "z-score is unitless.",
      ],
    },
    {
      slug: "probability-09-normal-approx-binomial",
      title: "Normal Approximation to the Binomial",
      description:
        "For large n, $\\mathrm{Bin}(n,p)$ looks normal with mean np and variance np(1-p).",
      category: probability.title,
      difficulty: 1,
      xpReward: 170,
      order: 9,
      type: "theory",
      theoryContent: md(
        "## The approximation",
        "If $X\\sim\\mathrm{Bin}(n,p)$ and $n$ is large (with $p$ not extreme), then",
        "$$X\\approx \\mathcal{N}(np,\\ np(1-p)).$$",
        "Standardize:",
        "$$Z\\approx \\frac{X-np}{\\sqrt{np(1-p)}}.$$",
        "### Interactive lab: compare discrete vs normal intuition",
        interactive({ kind: "binomial", title: "Binomial PMF", n: 40, p: 0.4 }),
        interactive({
          kind: "normalCdf",
          title: "Normal CDF with matching mean/variance (continuous intuition)",
          mu: 40 * 0.4,
          sigma: Math.sqrt(40 * 0.4 * 0.6),
          x: 16,
        }),
        "### Quick checks",
        "- When does normal approximation fail?\n- Why does the variance contain $p(1-p)$?\n- What is a continuity correction and why might it help?",
      ),
      hints: [
        "Approximation improves as np and n(1-p) grow.",
        "Binomial is sum of Bernoulli trials; variance adds.",
        "Continuity correction adjusts discrete-to-continuous mismatch.",
      ],
    },
    {
      slug: "probability-10-confidence-intervals-preview",
      title: "Confidence Intervals (Preview)",
      description: "Use normal quantiles (like 1.96) to build intervals for means/proportions.",
      category: probability.title,
      difficulty: 1,
      xpReward: 170,
      order: 10,
      type: "theory",
      theoryContent: md(
        "## The idea",
        "A (rough) 95% interval for a normal mean (known $\\sigma$) looks like",
        "$$\\bar X\\pm 1.96\\,\\frac{\\sigma}{\\sqrt{n}}.$$",
        "For a Bernoulli proportion, a common approximation is",
        "$$\\hat p\\pm 1.96\\sqrt{\\frac{\\hat p(1-\\hat p)}{n}}.$$",
        "### Interactive lab: z-critical value intuition",
        "Compute $\\Phi(1.96)$ and interpret the two-sided coverage.",
        interactive({
          kind: "normalCdf",
          title: "Why 1.96 gives ~95% two-sided",
          mu: 0,
          sigma: 1,
          x: 1.96,
        }),
        "### Quick checks",
        "- Why does width shrink like $1/\\sqrt{n}$?\n- What assumptions are hidden in using 1.96?\n- What is the difference between “confidence” and “probability the parameter is in the interval”?",
      ),
      hints: [
        "Standard error scales as 1/sqrt(n).",
        "The 1.96 comes from Φ(1.96)≈0.975 (one-sided).",
        "Confidence is about procedure long-run coverage, not Bayesian probability.",
      ],
    },
  ];

  return {
    generatedAt: new Date().toISOString(),
    courses: [
      { course: linearAlgebra, challenges: laModules },
      { course: calculus1, challenges: calcModules },
      { course: probability, challenges: probModules },
    ],
  };
}

async function seedToConvex(bundle: GeneratedBundle) {
  const url = process.env.NEXT_PUBLIC_CONVEX_URL;
  if (!url) {
    throw new Error("Missing NEXT_PUBLIC_CONVEX_URL in .env.local (required for --seed).");
  }

  const client = new ConvexHttpClient(url);

  for (const entry of bundle.courses) {
    const courseId = await client.mutation(api.seedCourses.createCourse, entry.course);
    for (const ch of entry.challenges) {
      await client.mutation(api.seedCourses.createChallenge, {
        ...ch,
        courseId,
      });
    }
  }
}

async function main() {
  const bundle = buildBundle();

  const outPath = path.join(process.cwd(), "scripts", "math_courses.json");
  fs.writeFileSync(outPath, JSON.stringify(bundle, null, 2));
  console.log(`✅ Wrote ${outPath}`);

  const seed = process.argv.includes("--seed");
  if (seed) {
    console.log("🌱 Seeding to Convex...");
    await seedToConvex(bundle);
    console.log("🏁 Done.");
  } else {
    console.log("ℹ️  To seed into Convex: `bun scripts/gen-math.ts --seed`");
  }
}

main().catch((err) => {
  console.error("❌ gen-math failed:", err);
  process.exit(1);
});
