{
  "course": {
    "title": "Algorithms & Data Structures",
    "description": "Master the fundamental algorithms and data structures that power computer science, from sorting and searching to trees and graphs.",
    "slug": "algorithms-data-structures",
    "order": 10,
    "icon": "Binary"
  },
  "challenges": [
    {
      "order": 1,
      "title": "Big O (Time/Space)",
      "category": "Algorithms & Data Structures",
      "type": "theory",
      "slug": "big-o-time-space",
      "xpReward": 50,
      "description": "Understand how to analyze the efficiency of algorithms using Big O notation for both time and space complexity.",
      "theoryContent": "### Introduction to Big O Notation\nBig O notation is a mathematical notation that describes the limiting behavior of a function when the argument tends towards a particular value or infinity. In computer science, it is used to classify algorithms according to how their running time or space requirements grow as the input size grows. We typically care about the worst-case scenario.\n\n#### Definition of Big O\nA function $f(n)$ is said to be $O(g(n))$ (read as \"f of n is big-O of g of n\") if there exist positive constants $c$ and $n_0$ such that $0 \\le f(n) \\le c \\cdot g(n)$ for all $n \\ge n_0$. This means that $g(n)$ is an asymptotic upper bound for $f(n)$.\n$$ f(n) \\in O(g(n)) \\iff \\exists c > 0, n_0 > 0 \\text{ s.t. } 0 \\le f(n) \\le c \\cdot g(n) \\text{ for all } n \\ge n_0 $$\n\n#### Common Time Complexities\nHere are some common orders of growth from fastest to slowest:\n*   **$O(1)$ - Constant Time:** The execution time does not depend on the input size $n$. Example: Accessing an array element by index.\n*   **$O(\\log n)$ - Logarithmic Time:** The execution time grows logarithmically with $n$. Example: Binary search.\n*   **$O(n)$ - Linear Time:** The execution time grows linearly with $n$. Example: Traversing a list.\n*   **$O(n \\log n)$ - Log-linear Time:** Often seen in efficient sorting algorithms. Example: Merge Sort, Quick Sort (average case).\n*   **$O(n^2)$ - Quadratic Time:** The execution time is proportional to the square of $n$. Example: Bubble Sort, Insertion Sort, selection sort.\n*   **$O(2^n)$ - Exponential Time:** The execution time doubles with each addition to $n$. Example: Solving the Towers of Hanoi problem recursively.\n*   **$O(n!)$ - Factorial Time:** Extremely slow, typically for brute-force solutions to problems like the Traveling Salesperson Problem.\n\n#### Deriving Big O for Algorithms\n\n**Example 1: Single Loop**\nConsider a simple loop that iterates $n$ times:\n```python\nfor i in range(n):\n    print(i)\n```\nThe `print(i)` operation takes constant time, say $k_1$. The loop runs $n$ times. So, the total time $T(n) = k_1 \\cdot n$. By the definition of Big O, $T(n) \\in O(n)$ (we can choose $c = k_1$ and $n_0 = 1$).\n\n**Example 2: Nested Loops**\n```python\nfor i in range(n):\n    for j in range(n):\n        print(i, j)\n```\nThe inner loop runs $n$ times for each iteration of the outer loop. The outer loop runs $n$ times. Thus, the total number of `print` operations is $n \\times n = n^2$. The time complexity is $O(n^2)$.\nThis can be formally shown as $\\sum_{i=1}^{n} \\sum_{j=1}^{n} O(1) = \\sum_{i=1}^{n} O(n) = O(n^2)$.\n\n**Example 3: Sum of distinct complexities**\nIf an algorithm consists of steps with different complexities, the highest order term dominates. For instance, if an algorithm has a running time $T(n) = 5n^2 + 100n + 500$, then $T(n) \\in O(n^2)$. This is because as $n$ grows very large, the $n^2$ term will contribute significantly more than the $n$ or constant terms. We express this as $T(n) = O(\\max(n^2, n, 1)) = O(n^2)$.\n\n#### Space Complexity\nSpace complexity refers to the amount of memory an algorithm needs to run to completion. This includes the input size, auxiliary space used for variables, data structures, and the recursion stack. We usually focus on auxiliary space, which is the extra space beyond the input itself.\n\nFor example, the `sum_array` function below takes $O(1)$ auxiliary space because it only uses a few variables (`total`, `x`) regardless of the array size $n$.\n```python\ndef sum_array(arr):\n    total = 0\n    for x in arr:\n        total += x\n    return total\n```\nIf an algorithm creates a copy of an input array of size $n$, its space complexity would be $O(n)$.\n\n#### Other Asymptotic Notations\n*   **Big Omega ($\\Omega$) Notation:** Provides an asymptotic lower bound. $f(n) \\in \\Omega(g(n))$ if there exist positive constants $c$ and $n_0$ such that $0 \\le c \\cdot g(n) \\le f(n)$ for all $n \\ge n_0$. This means $g(n)$ is an asymptotic lower bound for $f(n)$.\n*   **Big Theta ($\\Theta$) Notation:** Provides an asymptotically tight bound. $f(n) \\in \\Theta(g(n))$ if $f(n) \\in O(g(n))$ and $f(n) \\in \\Omega(g(n))$. This means $f(n)$ grows at the same rate as $g(n)$.\n$$ T(n) = \\Theta(g(n)) \\iff c_1 g(n) \\le T(n) \\le c_2 g(n) \\text{ for all } n \\ge n_0 $$",
      "difficulty": 1
    },
    {
      "xpReward": 75,
      "slug": "sorting-bubble-sort",
      "order": 2,
      "category": "Algorithms & Data Structures",
      "title": "Sorting: Bubble Sort",
      "type": "theory",
      "description": "Explore the simplest comparison-based sorting algorithm, Bubble Sort, and analyze its time complexity.",
      "theoryContent": "### Introduction to Bubble Sort\nBubble Sort is a simple comparison-based sorting algorithm. It works by repeatedly stepping through the list, comparing adjacent elements, and swapping them if they are in the wrong order. The pass through the list is repeated until no swaps are needed, which indicates that the list is sorted.\n\n#### How it Works\nThe algorithm repeatedly passes through the list, compares each pair of adjacent items, and swaps them if they are in the wrong order. Elements of greater value 'bubble' to the end of the list. In each pass, the largest unsorted element is placed at its correct position.\n\n#### Pseudocode\n```\nfunction bubbleSort(array A)\n    n = length(A)\n    for i from 0 to n-2\n        // Last i elements are already in place\n        for j from 0 to n-i-2\n            // Traverse the array from 0 to n-i-2\n            // Swap if the element found is greater than the next element\n            if A[j] > A[j+1]\n                swap(A[j], A[j+1])\n```\n\n#### Example Trace\nLet's trace Bubble Sort with the array `[5, 1, 4, 2, 8]`.\n\n**First Pass (i=0):**\n*   (5, 1) -> (1, 5), 4, 2, 8\n*   (1, 5, 4) -> 1, (4, 5), 2, 8\n*   (1, 4, 5, 2) -> 1, 4, (2, 5), 8\n*   (1, 4, 2, 5, 8) -> 1, 4, 2, (5, 8)\nArray after First Pass: `[1, 4, 2, 5, 8]` (8 is in its final position)\n\n**Second Pass (i=1):**\n*   (1, 4, 2, 5) -> 1, (2, 4), 5\n*   (1, 2, 4, 5) -> 1, 2, (4, 5)\nArray after Second Pass: `[1, 2, 4, 5, 8]` (5 is in its final position)\n\n**Third Pass (i=2):**\n*   (1, 2, 4) -> (1, 2), 4\n*   (1, 2, 4) -> 1, (2, 4)\nArray after Third Pass: `[1, 2, 4, 5, 8]` (4 is in its final position)\n\nNo swaps needed in the fourth pass, so the array is sorted.\n\n#### Time Complexity Analysis\n*   **Worst-Case and Average-Case:**\n    The outer loop runs $n-1$ times. The inner loop runs $n-1$ times in the first iteration, then $n-2$ times, and so on, down to 1 time.\n    The total number of comparisons (and potential swaps) can be represented by the sum of an arithmetic series:\n    $$ (n-1) + (n-2) + \\dots + 1 = \\sum_{k=1}^{n-1} k = \\frac{(n-1)n}{2} $$\n    This sum simplifies to $\\frac{n^2 - n}{2}$, which is proportional to $n^2$. Therefore, the worst-case and average-case time complexity of Bubble Sort is $O(n^2)$.\n\n*   **Best-Case:**\n    If the array is already sorted, and an optimization is added to stop the algorithm if no swaps are made in a pass, Bubble Sort can complete in $O(n)$ time. This is because it would make one full pass ($n-1$ comparisons) and detect no swaps, then terminate.\n    The number of passes needed is at most $n-1$. In each pass $i$ (0-indexed), the number of comparisons is $n-i-1$. So, for the first pass ($i=0$), there are $n-1$ comparisons.\n\n#### Space Complexity\nBubble Sort is an in-place sorting algorithm, meaning it only requires a constant amount of additional memory for temporary variables (like for swapping). Thus, its space complexity is $O(1)$.\n\n#### Stability\nBubble Sort is a **stable** sorting algorithm. This means that if two elements have equal values, their relative order in the sorted array will be the same as in the original array.",
      "difficulty": 2
    },
    {
      "slug": "sorting-merge-sort",
      "order": 3,
      "title": "Sorting: Merge Sort",
      "category": "Algorithms & Data Structures",
      "difficulty": 3,
      "description": "Delve into Merge Sort, a divide-and-conquer sorting algorithm known for its consistent $O(n \\log n)$ performance.",
      "theoryContent": "### Introduction to Merge Sort\nMerge Sort is an efficient, comparison-based sorting algorithm. It is a divide-and-conquer algorithm. It works by dividing an unsorted list into $n$ sublists, each containing one element (a list of one element is considered sorted). Then, it repeatedly merges sublists to produce new sorted sublists until there is only one sublist remaining, which is the sorted list.\n\n#### How it Works (Divide and Conquer)\n1.  **Divide:** Divide the unsorted list into two sublists of about half the size.\n2.  **Conquer:** Sort each sublist recursively by repeatedly applying the divide and conquer steps.\n3.  **Combine:** Merge the two sorted sublists back into one sorted list.\n\n#### Pseudocode\n```\nfunction mergeSort(array A, p, r)\n    if p < r\n        q = floor((p+r)/2)  // Find the middle point\n        mergeSort(A, p, q)       // Sort the first half\n        mergeSort(A, q+1, r)     // Sort the second half\n        merge(A, p, q, r)        // Merge the sorted halves\n\nfunction merge(array A, p, q, r)\n    n1 = q - p + 1\n    n2 = r - q\n\n    // Create temporary arrays L and R\n    create L[n1+1], R[n2+1]\n    for i from 0 to n1-1\n        L[i] = A[p+i]\n    for j from 0 to n2-1\n        R[j] = A[q+1+j]\n\n    L[n1] = infinity  // Sentinel to avoid checking for empty arrays\n    R[n2] = infinity\n\n    i = 0\n    j = 0\n    for k from p to r\n        if L[i] <= R[j]\n            A[k] = L[i]\n            i = i + 1\n        else\n            A[k] = R[j]\n            j = j + 1\n```\n\n#### Time Complexity Analysis\nMerge Sort's time complexity can be analyzed using a recurrence relation.\n\n**Recurrence Relation:**\n$T(n)$ is the time to sort an array of size $n$. The algorithm divides the array into two halves, each taking $T(n/2)$ time to sort. The merging step takes $O(n)$ time.\n$$ T(n) = 2T(n/2) + O(n) $$\n\n**Solving the Recurrence using Master Theorem:**\nThe Master Theorem applies to recurrences of the form $T(n) = aT(n/b) + f(n)$.\nIn our case, $a=2$, $b=2$, and $f(n)=n$.\nWe compare $f(n)$ with $n^{\\log_b a}$.\n$n^{\\log_b a} = n^{\\log_2 2} = n^1 = n$.\nSince $f(n) = n = \\Theta(n^{\\log_b a})$, this falls under Case 2 of the Master Theorem.\nTherefore, the solution is $T(n) = \\Theta(n^{\\log_b a} \\log n) = \\Theta(n^1 \\log n) = \\Theta(n \\log n)$.\n\nThis means Merge Sort has a time complexity of $O(n \\log n)$ in the worst-case, best-case, and average-case scenarios. This consistent performance is one of its key advantages. The depth of the recursion tree is $\\log_2 n$, and at each level of the recursion, the total work done for merging is $O(n)$. The total number of comparisons in the worst case is $n \\log_2 n - n + 1$.\n\n#### Space Complexity\nMerge Sort requires auxiliary space for the temporary arrays used in the `merge` step. In the worst case, these temporary arrays can hold up to $n$ elements. Thus, the space complexity of Merge Sort is $O(n)$.\n\n#### Stability\nMerge Sort is a **stable** sorting algorithm. This is because the `merge` operation can be implemented to preserve the relative order of equal elements. When comparing `L[i]` and `R[j]`, if `L[i] <= R[j]`, we always pick from `L[i]` first, ensuring stability.",
      "xpReward": 125,
      "type": "theory"
    },
    {
      "slug": "sorting-quick-sort",
      "order": 4,
      "difficulty": 4,
      "title": "Sorting: Quick Sort",
      "type": "theory",
      "xpReward": 175,
      "description": "Uncover the intricacies of Quick Sort, another divide-and-conquer algorithm renowned for its average-case performance.",
      "theoryContent": "### Introduction to Quick Sort\nQuick Sort is an efficient, in-place, comparison-based sorting algorithm. It is a divide-and-conquer algorithm that works by selecting a 'pivot' element from the array and partitioning the other elements into two sub-arrays, according to whether they are less than or greater than the pivot. The sub-arrays are then sorted recursively. The term 'quick sort' comes from the fact that, in its average case, it is very efficient.\n\n#### How it Works (Divide and Conquer)\n1.  **Pick a pivot:** Select an element from the array, called the pivot.\n2.  **Partition:** Rearrange the array such that all elements less than the pivot come before it, and all elements greater than the pivot come after it. Elements equal to the pivot can go on either side. After this partitioning, the pivot is in its final sorted position.\n3.  **Recursively sort:** Recursively apply the above two steps to the sub-array of elements with smaller values and separately to the sub-array of elements with greater values.\n\n#### Pseudocode (Lomuto Partition Scheme)\n```\nfunction quickSort(array A, p, r)\n    if p < r\n        q = partition(A, p, r)  // Partition the array around a pivot\n        quickSort(A, p, q-1)      // Recursively sort elements before pivot\n        quickSort(A, q+1, r)      // Recursively sort elements after pivot\n\nfunction partition(array A, p, r)\n    x = A[r]  // Choose the last element as the pivot\n    i = p - 1   // Index of smaller element\n    for j from p to r-1\n        if A[j] <= x\n            i = i + 1\n            swap(A[i], A[j])\n    swap(A[i+1], A[r])\n    return i + 1  // Return the pivot's final position\n```\n\n#### Time Complexity Analysis\nQuick Sort's performance highly depends on the choice of the pivot element.\n\n*   **Worst-Case:**\n    This occurs when the pivot always results in highly unbalanced partitions (e.g., the pivot is always the smallest or largest element). This happens if the array is already sorted (or reverse sorted) and the last element is always chosen as the pivot. In this scenario, one sub-array will be empty, and the other will contain $n-1$ elements.\n    The recurrence relation for the worst-case is:\n    $$ T(n) = T(0) + T(n-1) + O(n) \\implies T(n) = T(n-1) + O(n) $$\n    Expanding this, we get $T(n) = O(n) + O(n-1) + \\dots + O(1) = \\sum_{k=1}^n O(k)$, which evaluates to $O(n^2)$.\n\n*   **Best-Case:**\n    This occurs when the pivot always divides the array into two roughly equal halves. For example, if the pivot is always the median element.\n    The recurrence relation for the best-case is:\n    $$ T(n) = 2T(n/2) + O(n) $$\n    By the Master Theorem (Case 2), this recurrence solves to $T(n) = \\Theta(n \\log n)$.\n\n*   **Average-Case:**\n    The average-case time complexity of Quick Sort is also $O(n \\log n)$. This is a significant result and is why Quick Sort is one of the most widely used sorting algorithms. The proof for the average case typically involves probabilistic analysis, especially when using a randomized pivot selection strategy (e.g., choosing a random element as the pivot).\n    The expected running time for randomized Quick Sort is $E[T(n)] = O(n \\log n)$.\n\n#### Space Complexity\nQuick Sort is generally considered an in-place sorting algorithm because it sorts by moving elements within the array and doesn't require extra arrays like Merge Sort. However, it uses recursion, which consumes space on the call stack.\n*   **Average-Case:** The depth of the recursion tree is $O(\\log n)$, leading to $O(\\log n)$ space complexity.\n*   **Worst-Case:** In the worst-case (skewed partitions), the recursion depth can be $O(n)$, resulting in $O(n)$ space complexity.\n\n#### Stability\nQuick Sort, as typically implemented (like the Lomuto or Hoare partition schemes), is **not a stable** sorting algorithm. Elements with equal values may not preserve their relative order after sorting.",
      "category": "Algorithms & Data Structures"
    },
    {
      "difficulty": 4,
      "category": "Algorithms & Data Structures",
      "theoryContent": "### Introduction to Binary Search Trees (BST)\nA Binary Search Tree (BST), also known as an ordered or sorted binary tree, is a node-based binary tree data structure that satisfies the binary search tree property for every node.\n\n#### BST Property\nFor each node $x$ in a BST:\n1.  All keys in its left subtree $L_x$ must be less than the key of $x$. Formally: $k \\in L_x \\implies k < \\text{key}(x)$.\n2.  All keys in its right subtree $R_x$ must be greater than the key of $x$. Formally: $k \\in R_x \\implies k > \\text{key}(x)$.\n3.  Both the left and right subtrees must also be binary search trees themselves.\n4.  There are typically no duplicate keys allowed in a BST (or a specific rule is defined for handling them, e.g., putting them in the right subtree).\n\nThis property allows for efficient searching, insertion, and deletion of elements.\n\n#### Basic Operations and Complexities\nLet $h$ be the height of the tree and $n$ be the number of nodes.\n\n1.  **Search (lookup):**\n    To search for a key, start at the root. If the key is equal to the node's key, the search is successful. If the key is less than the node's key, go to the left child. If the key is greater, go to the right child. Repeat until the key is found or a null pointer is encountered.\n    *   **Time Complexity:** $O(h)$.\n    *   **Worst-case (skewed tree, like a linked list):** $O(n)$.\n    *   **Best-case (balanced tree):** $O(\\log n)$.\n\n2.  **Insertion:**\n    To insert a new key, first search for it. If the key is found, it's either not inserted (if duplicates are not allowed) or handled according to a specific rule. If the key is not found, the search will terminate at a null pointer. The new node is then inserted as a child of the last visited node.\n    *   **Time Complexity:** $O(h)$.\n\n3.  **Deletion:**\n    Deleting a node from a BST is more complex as there are three cases:\n    *   **Case 1: Node has no children (is a leaf):** Simply remove the node.\n    *   **Case 2: Node has one child:** Replace the node with its child.\n    *   **Case 3: Node has two children:** Find the inorder successor (the smallest node in the right subtree) or inorder predecessor (the largest node in the left subtree) of the node to be deleted. Copy its value to the node to be deleted, then recursively delete the successor/predecessor.\n    *   **Time Complexity:** $O(h)$.\n\n#### Height of a BST\nThe efficiency of BST operations heavily depends on the height of the tree. The height of a BST can vary significantly:\n*   **Minimum height:** For a perfectly balanced BST with $n$ nodes, the height $h = \\lfloor \\log_2 n \\rfloor + 1$. The number of nodes $n$ and height $h$ are related by $n \\approx 2^h - 1$, so $h \\approx \\log_2 (n+1)$. In this ideal scenario, all operations are $O(\\log n)$.\n*   **Maximum height (worst-case):** If nodes are inserted in strictly increasing or decreasing order, the BST can degenerate into a skewed tree, effectively becoming a linked list. In this case, the height $h = n$, and operations take $O(n)$ time.\n\n#### Traversal Methods\nBSTs support various traversal methods, with Inorder traversal being particularly useful for retrieving elements in sorted order.\n*   **Inorder Traversal:** Visits the left subtree, then the root, then the right subtree ($L \\to \\text{Root} \\to R$). An inorder traversal of a BST yields elements in non-decreasing order.\n*   **Preorder Traversal:** Visits the root, then the left subtree, then the right subtree ($\text{Root} \\to L \\to R$).\n*   **Postorder Traversal:** Visits the left subtree, then the right subtree, then the root ($L \\to R \\to \\text{Root}$). \n\n#### Self-Balancing BSTs\nTo guarantee $O(\\log n)$ performance for all operations, self-balancing BSTs are used. These trees automatically adjust their structure to maintain a balanced height after insertions and deletions. Examples include AVL trees and Red-Black trees.\n\n#### Number of Distinct BSTs\nThe number of structurally distinct binary search trees that can be formed with $n$ distinct keys is given by the $n$-th Catalan number, denoted as $C_n$.\n$$ C_n = \\frac{1}{n+1} \\binom{2n}{n} $$",
      "type": "theory",
      "slug": "trees-binary-search-trees",
      "title": "Trees: Binary Search Trees (BST)",
      "xpReward": 200,
      "order": 5,
      "description": "Explore the fundamental data structure of Binary Search Trees, enabling efficient search, insertion, and deletion operations."
    },
    {
      "order": 6,
      "xpReward": 250,
      "title": "Graphs: BFS and DFS",
      "category": "Algorithms & Data Structures",
      "type": "theory",
      "description": "Dive into Breadth-First Search (BFS) and Depth-First Search (DFS), two fundamental algorithms for traversing and searching graph data structures.",
      "difficulty": 5,
      "slug": "graphs-bfs-dfs",
      "theoryContent": "### Introduction to Graph Traversal\nA graph $G = (V, E)$ is a non-linear data structure consisting of a set of vertices (or nodes) $V$ and a set of edges (or arcs) $E$ connecting pairs of vertices. Graph traversal algorithms systematically visit every vertex and edge in a graph.\n\nGraph representations:\n*   **Adjacency List:** An array of lists where the $i$-th element stores a list of vertices adjacent to vertex $i$. Space complexity: $O(|V|+|E|)$.\n*   **Adjacency Matrix:** A 2D array of size $|V| \\times |V|$ where `adj[i][j]` is 1 if there's an edge from vertex $i$ to vertex $j$, and 0 otherwise. Space complexity: $O(|V|^2)$.\n\n### Breadth-First Search (BFS)\nBFS is an algorithm for traversing or searching tree or graph data structures. It starts at the tree root (or some arbitrary node of a graph, sometimes referred to as a 'search key') and explores all of the neighbor nodes at the present depth before moving on to the nodes at the next depth level.\n\n#### How it Works\nBFS uses a queue data structure to keep track of the nodes to visit. It proceeds layer by layer from the starting node.\n\n#### Pseudocode\n```\nBFS(Graph G, Node start_node):\n    create a queue Q\n    enqueue start_node\n    mark start_node as visited\n    while Q is not empty:\n        current_node = dequeue Q\n        // Process current_node (e.g., print it)\n        for each neighbor of current_node:\n            if neighbor is not visited:\n                mark neighbor as visited\n                enqueue neighbor\n```\n\n#### Time and Space Complexity\n*   **Time Complexity:** $O(|V| + |E|)$ when using an adjacency list representation. Each vertex is enqueued and dequeued once, and each edge is examined once.\n*   **Space Complexity:** $O(|V|)$ in the worst case, as the queue might store all vertices in a dense graph (e.g., a star graph).\n\n#### Applications\n*   Finding the shortest path in an unweighted graph (number of edges). For a source $s$ and a target $v$, the distance is $\text{dist}(s,v)$.\n*   Finding connected components.\n*   Network broadcasting.\n\n### Depth-First Search (DFS)\nDFS is an algorithm for traversing or searching tree or graph data structures. The algorithm starts at the root (or some arbitrary node) and explores as far as possible along each branch before backtracking.\n\n#### How it Works\nDFS uses a stack data structure (either explicitly or implicitly through recursion) to keep track of the nodes to visit. It dives deep into a path before exploring other branches.\n\n#### Pseudocode (Iterative using a stack)\n```\nDFS(Graph G, Node start_node):\n    create a stack S\n    push start_node onto S\n    mark start_node as visited\n    while S is not empty:\n        current_node = pop S\n        // Process current_node (e.g., print it)\n        for each neighbor of current_node:\n            if neighbor is not visited:\n                mark neighbor as visited\n                push neighbor onto S\n```\n\n#### Pseudocode (Recursive)\n```\nDFS_Recursive(Graph G, Node u):\n    mark u as visited\n    // Process u\n    for each neighbor v of u:\n        if v is not visited:\n            DFS_Recursive(G, v)\n```\n\n#### Time and Space Complexity\n*   **Time Complexity:** $O(|V| + |E|)$ when using an adjacency list representation. Each vertex is visited once, and each edge is explored once.\n*   **Space Complexity:** $O(|V|)$ in the worst case, for the recursion stack (if recursive) or the explicit stack (if iterative). This happens in a skewed graph (e.g., a linked list).\n\n#### Applications\n*   Cycle detection in graphs.\n*   Topological sorting of a Directed Acyclic Graph (DAG).\n*   Finding strongly connected components.\n*   Solving puzzles with a single solution, such as mazes (where a specific path needs to be found)."
    }
  ]
}
